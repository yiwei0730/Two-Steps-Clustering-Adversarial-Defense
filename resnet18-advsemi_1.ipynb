{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow, imsave\n",
    "from sklearn.metrics import confusion_matrix, recall_score, classification_report, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from semi.MCMC_loss import DBI, margin\n",
    "model_path = './Model_pkl/advsemi_nocetrans_100.pkl'\n",
    "\n",
    "from utils import *\n",
    "torch.cuda.set_device(1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:1\" if use_cuda else \"cpu\")\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch-size', default=256, type=int)\n",
    "parser.add_argument('--data-dir', default='../../cifar-data', type=str)\n",
    "parser.add_argument('--epochs', default=80, type=int)\n",
    "parser.add_argument('--lr-schedule', default='cyclic', type=str, choices=['cyclic', 'multistep'])\n",
    "parser.add_argument('--lr-min', default=0., type=float)\n",
    "parser.add_argument('--lr-max', default=0.01, type=float)\n",
    "parser.add_argument('--weight-decay', default=5e-4, type=float)\n",
    "parser.add_argument('--momentum', default=0.9, type=float)\n",
    "parser.add_argument('--epsilon', default=8, type=int)\n",
    "parser.add_argument('--attack-iters', default=10, type=int, help='Attack iterations')\n",
    "parser.add_argument('--restarts', default=1, type=int)\n",
    "parser.add_argument('--alpha', default=2, type=int, help='Step size') #pgd\n",
    "#parser.add_argument('--alpha', default=10, type=int, help='Step size') #fast\n",
    "parser.add_argument('--delta-init', default='random', choices=['zero', 'random'],\n",
    "    help='Perturbation initialization method')\n",
    "parser.add_argument('--out-dir', default='train_pgd_output', type=str, help='Output directory')\n",
    "parser.add_argument('--seed', default=0, type=int, help='Random seed')\n",
    "parser.add_argument('--opt-level', default='O1', type=str, choices=['O0', 'O1', 'O2'],\n",
    "    help='O0 is FP32 training, O1 is Mixed Precision, and O2 is \"Almost FP16\" Mixed Precision')\n",
    "parser.add_argument('--loss-scale', default='1.0', type=str, choices=['1.0', 'dynamic'],\n",
    "    help='If loss_scale is \"dynamic\", adaptively adjust the loss scale over time')\n",
    "parser.add_argument('--master-weights', action='store_true',\n",
    "    help='Maintain FP32 master weights to accompany any FP16 model weights, not applicable for O1 opt level')\n",
    "args = parser.parse_args(args=['--batch-size', '256'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "mu = torch.tensor(cifar10_mean).view(3,1,1).cuda()\n",
    "std = torch.tensor(cifar10_std).view(3,1,1).cuda()\n",
    "\n",
    "epsilon = (args.epsilon / 255.) / std\n",
    "alpha = (args.alpha / 255.) / std\n",
    "pgd_alpha = (2 / 255.) / std\n",
    "\n",
    "upper_limit = ((1 - mu)/ std)\n",
    "lower_limit = ((0 - mu)/ std)\n",
    "def clamp(X, lower_limit, upper_limit):\n",
    "    return torch.max(torch.min(X, upper_limit), lower_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset, trainloader, testset, testloader, devset, devloader = read_cifar10()\n",
    "trainset, trainloader, testset, testloader = read_SVHN()\n",
    "from Resnet18 import *\n",
    "model = ResNet18(pre_train=False).to(device)\n",
    "modeling = model_1(model).to(device) #Pretrain = False\n",
    "model.load_state_dict(torch.load('./Model_pkl/warm-fast.pkl'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apex.amp as amp\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr_max, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "opt = optim.Adam(model.parameters(),lr=1e-3)\n",
    "amp_args = dict(opt_level=args.opt_level, loss_scale=args.loss_scale, verbosity=False)\n",
    "if args.opt_level == 'O2':\n",
    "    amp_args['master_weights'] = args.master_weights\n",
    "model, optimizer = amp.initialize(model, optimizer, **amp_args)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "#lr_steps = args.epochs * len(trainloader)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=args.lr_max, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[55, 70],gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_MCMC(modeling,inputs, labels,ul_inputs, optimizer,train_correct):\n",
    "def train_MCMC(modeling, inputs, labels, ul_inputs, optimizer):\n",
    "    cuda_gpu = torch.cuda.is_available()\n",
    "    z_l,outputs = modeling(inputs)\n",
    "    ce_loss = criterion(outputs,labels)\n",
    "    z_ul,ul_y = modeling(ul_inputs)\n",
    "    #print(outputs)\n",
    "    loss_DB = DBI(10, 512, cuda_gpu)\n",
    "    loss_ML = margin(10, 10, cuda_gpu)\n",
    "    label_DB_loss = loss_DB(z_l, labels).to(device)\n",
    "    #print(label_DB_loss)\n",
    "    _, pre_ul_y = torch.max(ul_y, 1)\n",
    "\n",
    "    total_DB_loss = loss_DB(z_ul, pre_ul_y).to(device)\n",
    "\n",
    "    z_ul = z_ul.type(torch.float)\n",
    "\n",
    "    margin_loss = loss_ML(z_ul, pre_ul_y).to(device)\n",
    "    \n",
    "    #loss = 3*ce_loss+ 0.5*label_DB_loss + 0.5*total_DB_loss + 1*margin_loss\n",
    "    loss = label_DB_loss + 10*total_DB_loss + margin_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    #loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #train_correct += (torch.max(outputs,1)[1]==labels).sum().item()\n",
    "    del loss_DB, loss_ML\n",
    "    return ce_loss,label_DB_loss, total_DB_loss, margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training(model, inputs, labels, optimizer, epsilon, lower_limit, upper_limit, train_correct):\n",
    "    delta = torch.zeros_like(inputs).to(device)\n",
    "    for i in range(len(epsilon)):\n",
    "        delta[:, i, :, :].uniform_(-epsilon[i][0][0].item(), epsilon[i][0][0].item())\n",
    "    delta.data = clamp(delta, lower_limit - inputs, upper_limit - inputs)\n",
    "    delta.requires_grad = True\n",
    "    for _ in range(args.attack_iters):\n",
    "        outputs = model(inputs + delta)\n",
    "        loss = criterion(outputs, labels)\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        #loss.backward()\n",
    "        grad = delta.grad.detach()\n",
    "        delta.data = clamp(delta + alpha * torch.sign(grad), -epsilon, epsilon)\n",
    "        delta.data = clamp(delta, lower_limit - inputs, upper_limit - inputs)\n",
    "        delta.grad.zero_()\n",
    "    delta = delta.detach()\n",
    "    outputs = model(inputs + delta)\n",
    "    adv_loss = criterion(outputs,labels)\n",
    "    loss = 1*adv_loss\n",
    "    optimizer.zero_grad()\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    #loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_correct += (torch.max(outputs,1)[1]==labels).sum().item()\n",
    "    \n",
    "    return adv_loss, train_correct, inputs + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss().to(device)\n",
    "tr_acc = 0\n",
    "train_size = len(trainset)\n",
    "test_size  = len(testset)\n",
    "#cuda_gpu = torch.cuda.is_available()\n",
    "prev_robust_acc  = 0\n",
    "pgd_all = []\n",
    "test_all = []\n",
    "#lr = 1e-3\n",
    "#epoch_decay_start = 50\n",
    "maxepoch = 0\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_correct  = 0\n",
    "    for j,data in tqdm(enumerate(trainloader)):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "         # adversarial training\n",
    "        adv_loss, train_correct, ul_inputs = adversarial_training(model, inputs, labels, optimizer, epsilon, lower_limit, upper_limit, train_correct)\n",
    "        #ce_loss, label_DB_loss, total_DB_loss, margin_loss, train_correct = train_MCMC(modeling, inputs, labels, ul_inputs, optimizer,train_correct)\n",
    "        ce_loss, label_DB_loss, total_DB_loss, margin_loss = train_MCMC(modeling, inputs, labels, ul_inputs, optimizer)\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        loss = 1*label_DB_loss + 10*total_DB_loss + 1*margin_loss + 1*adv_loss\n",
    "        #optimizer.step()\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    _, pgd_acc, _ = evaluate_pgd2(testloader, model, 20, 1)      \n",
    "    _, test_acc = evaluate_standard(testloader, model)\n",
    "    pgd_all.append(pgd_acc)\n",
    "    test_all.append(test_acc)\n",
    "    if pgd_acc > prev_robust_acc:\n",
    "        prev_robust_acc = pgd_acc\n",
    "        torch.save(model.state_dict(), './Model_pkl/advsemi_nocetrans_100_best.pkl')\n",
    "        maxepoch = epoch\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    train_loss = train_loss/train_size\n",
    "    tr_acc = train_correct*100.0/train_size\n",
    "    print(\"Epoch :\", epoch+1, \", CE Loss :\", ce_loss.item(), \", LDB Loss :\", label_DB_loss.item(),\n",
    "                      \", TDB Loss :\", total_DB_loss.item(),\", MM Loss :\", margin_loss.item(),\", adv Loss :\", adv_loss.item())\n",
    "    print(\"Epoch [%d/%d], accuracy=[%.2f], test accuracy=[%.4f], robust = [%.4f], Loss: %.4f, lr : %.4f\" \n",
    "          %(epoch+1, args.epochs, tr_acc, test_acc,pgd_acc, train_loss, lr))\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD-20 robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = evaluate_standard(trainloader, model)\n",
    "test_loss, test_acc = evaluate_standard(testloader, model)\n",
    "train_adv_loss, train_adv_acc, train_dist = evaluate_pgd2(trainloader, model, 20, 1)\n",
    "pgd_loss, pgd_acc, pgd_dist = evaluate_pgd2(testloader, model, 20, 1)\n",
    "\n",
    "\n",
    "print('train_loss = %.4f, train_acc =%.4f' %(train_loss, train_acc))\n",
    "print('test_loss = %.4f, test_acc =%.4f' %(test_loss, test_acc))\n",
    "print('train_adv_loss = %.4f, train_adv_acc =%.4f, train_dis = %.4f' %(train_adv_loss, train_adv_acc, train_dist ))\n",
    "print('pgd_loss = %.4f, pgd_acc = %.4f, pgd_dist = %.4f' %(pgd_loss, pgd_acc, pgd_dist))\n",
    "print(train_acc,\"|\",test_acc,\"|\",train_adv_acc,\"|\",pgd_acc,\"|\",pgd_loss,\"|\",pgd_dist,\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load('./Model_pkl/advsemi_nocetrans_100_best.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss, train_acc = evaluate_standard(trainloader, model)\n",
    "test_loss, test_acc = evaluate_standard(testloader, model)\n",
    "train_adv_loss, train_adv_acc, train_dist = evaluate_pgd2(trainloader, model, 20, 1)\n",
    "pgd_loss, pgd_acc, pgd_dist = evaluate_pgd2(testloader, model, 20, 1)\n",
    "\n",
    "\n",
    "print('train_loss = %.4f, train_acc =%.4f' %(train_loss, train_acc))\n",
    "print('test_loss = %.4f, test_acc =%.4f' %(test_loss, test_acc))\n",
    "print('train_adv_loss = %.4f, train_adv_acc =%.4f, train_dis = %.4f' %(train_adv_loss, train_adv_acc, train_dist ))\n",
    "print('pgd_loss = %.4f, pgd_acc = %.4f, pgd_dist = %.4f' %(pgd_loss, pgd_acc, pgd_dist))\n",
    "print(train_acc,\"|\",test_acc,\"|\",train_adv_acc,\"|\",pgd_acc,\"|\",pgd_loss,\"|\",pgd_dist,\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM white-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM robustness\n",
    "evaluate_pgd1(testloader, model, 1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PGD_plot and test_plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"ResNet Model\")\n",
    "x= np.arange(1,len(pgd_all)+1)\n",
    "plt.plot(x,pgd_all, label = 'PGD')\n",
    "plt.plot(x,test_all, label = 'Test')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"acc %\")\n",
    "plt.legend()\n",
    "plt.savefig('./image/advsemi_nocetrans_100.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW White-Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 20\n",
    "epsilon = 0.031\n",
    "step_size = 0.003\n",
    "\n",
    "from torch.autograd import Variable\n",
    "def eval_adv_test_whitebox_cw(model, device, testloader):\n",
    "    \"\"\"\n",
    "    evaluate model by white-box attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_robust = _cw_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "    print('cw robust_acc: ', 1 - robust_err_total / len(testloader.dataset))\n",
    "\n",
    "\n",
    "def _cw_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=0.031,\n",
    "                  num_steps=20,\n",
    "                  step_size=0.003\n",
    "                ):\n",
    "    # out = model(X)\n",
    "    # err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    \n",
    "    random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "    X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "    epsilon = epsilon / std\n",
    "    step_size = step_size / std\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            output = model(X_pgd)\n",
    "            correct_logit = torch.sum(torch.gather(output, 1, (y.unsqueeze(1)).long()).squeeze())\n",
    "            tmp1 = torch.argsort(output, dim=1)[:, -2:]\n",
    "            new_y = torch.where(tmp1[:, -1] == y, tmp1[:, -2], tmp1[:, -1])\n",
    "            wrong_logit = torch.sum(torch.gather(output, 1, (new_y.unsqueeze(1)).long()).squeeze())\n",
    "            loss = - F.relu(correct_logit-wrong_logit)\n",
    "        loss.backward()\n",
    "        eta = step_size * X_pgd.grad.data.sign()\n",
    "        X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        eta = clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        eta = clamp(eta, lower_limit - X.data, upper_limit - X.data)\n",
    "        X_pgd = Variable(X.data+eta, requires_grad= True)\n",
    "        #X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        #eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        #X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        #X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "    output= model(X_pgd)\n",
    "    err_pgd = (output.data.max(1)[1] != y.data).float().sum()\n",
    "    return err_pgd\n",
    "\n",
    "model.eval()\n",
    "eval_adv_test_whitebox_cw(model, device, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW BLACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def eval_adv_test_blackbox(model_target, model_source, device, testloader):\n",
    "    \"\"\"\n",
    "    evaluate model by black-box attack\n",
    "    \"\"\"\n",
    "    model_target.eval()\n",
    "    model_source.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust = _pgd_blackbox(model_target, model_source, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "    print('natural_err_total: ', natural_err_total)\n",
    "    print('robust_err_total: ', robust_err_total)\n",
    "    print('black-box robust_acc: ', 1 - robust_err_total / len(testloader.dataset))\n",
    "    print('%.2f/%.2f/%.2f'%(natural_err_total*100/ len(testloader.dataset),robust_err_total*100/ len(testloader.dataset),(1 - robust_err_total / len(testloader.dataset))*100))\n",
    "          \n",
    "def _pgd_blackbox(model_target,\n",
    "                  model_source,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=8/255,\n",
    "                  num_steps=20,\n",
    "                  step_size=1/255):\n",
    "    out = model_target(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "   \n",
    "    random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "    X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "    epsilon = epsilon / std\n",
    "    step_size = step_size / std\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "        with torch.enable_grad():\n",
    "            output = model_source(X_pgd)\n",
    "            correct_logit = torch.sum(torch.gather(output, 1, (y.unsqueeze(1)).long()).squeeze())\n",
    "            tmp1 = torch.argsort(output, dim=1)[:, -2:]\n",
    "            new_y = torch.where(tmp1[:, -1] == y, tmp1[:, -2], tmp1[:, -1])\n",
    "            wrong_logit = torch.sum(torch.gather(output, 1, (new_y.unsqueeze(1)).long()).squeeze())\n",
    "            loss = - F.relu(correct_logit-wrong_logit)\n",
    "        loss.backward()\n",
    "        eta = step_size * X_pgd.grad.data.sign()\n",
    "        X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        eta = clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        eta = clamp(eta, lower_limit - X.data, upper_limit - X.data)\n",
    "        X_pgd = Variable(X.data+eta, requires_grad= True)\n",
    "\n",
    "    err_pgd = (model_target(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    #print('err pgd black-box: ', err_pgd)\n",
    "    return err, err_pgd\n",
    "\n",
    "\n",
    "\n",
    "target_model_path = './Model_pkl/SVHN-100-3.pkl'\n",
    "source_model_path = './Model_pkl/resnet50-svhn.pkl'\n",
    "model_target = ResNet18(pre_train=False).to(device)\n",
    "model_target.load_state_dict(torch.load(target_model_path))\n",
    "model_source = ResNet50().to(device)\n",
    "model_source.load_state_dict(torch.load(source_model_path))\n",
    "\n",
    "eval_adv_test_blackbox(model_target, model_source, device, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD BLACK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def eval_adv_test_blackbox(model_target, model_source, device, testloader):\n",
    "    \"\"\"\n",
    "    evaluate model by black-box attack\n",
    "    \"\"\"\n",
    "    model_target.eval()\n",
    "    model_source.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust = _pgd_blackbox(model_target, model_source, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "    print('natural_err_total: ', natural_err_total)\n",
    "    print('robust_err_total: ', robust_err_total)\n",
    "    print('black-box robust_acc: ', 1 - robust_err_total / len(testloader.dataset))\n",
    "    print('%.2f/%.2f/%.2f'%(natural_err_total*100/ len(testloader.dataset),robust_err_total*100/ len(testloader.dataset),(1 - robust_err_total / len(testloader.dataset))*100))\n",
    "          \n",
    "def _pgd_blackbox(model_target,\n",
    "                  model_source,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=8/255,\n",
    "                  num_steps=20,\n",
    "                  step_size=1/255):\n",
    "    out = model_target(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "   \n",
    "    random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "    X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "    epsilon = epsilon / std\n",
    "    step_size = step_size / std\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model_source(X_pgd), y)\n",
    "        loss.backward()\n",
    "        eta = step_size * X_pgd.grad.data.sign()\n",
    "        X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        eta = clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        eta = clamp(eta, lower_limit - X.data, upper_limit - X.data)\n",
    "        X_pgd = Variable(X.data+eta, requires_grad= True)\n",
    "\n",
    "    err_pgd = (model_target(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    #print('err pgd black-box: ', err_pgd)\n",
    "    return err, err_pgd\n",
    "\n",
    "\n",
    "target_model_path = './Model_pkl/SVHN-100-3.pkl'\n",
    "source_model_path = './Model_pkl/resnet50-svhn.pkl'\n",
    "model_target = ResNet18(pre_train=False).to(device)\n",
    "model_target.load_state_dict(torch.load(target_model_path))\n",
    "model_source = ResNet50().to(device)\n",
    "model_source.load_state_dict(torch.load(source_model_path))\n",
    "\n",
    "eval_adv_test_blackbox(model_target, model_source, device, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = './Model_pkl/resnet50.pkl'\n",
    "#target_model_path = './Model_pkl/pgd-200.pkl'\n",
    "#target_model_path = './Model_pkl/advsemi_nocetrans_100-3-2-3.pkl'\n",
    "#target_model_path = './Model_pkl/TRADES-ori.pkl'\n",
    "#target_model_path = './Model_pkl/TRADES-noce-2.pkl'\n",
    "#target_model_path = './Model_pkl/MART-ori.pkl'\n",
    "#target_model_path = './Model_pkl/MART-noce-3.pkl'\n",
    "\n",
    "#model_path = './Model_pkl/resnet50-svhn.pkl'\n",
    "#target_model_path = './Model_pkl/SVHN-100-3_best.pkl'\n",
    "#target_model_path = './Model_pkl/SVHN-MART-1_best.pkl'\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Testing in confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eval test , and plot CM\n",
    "def eval_model(model,dev_loader):\n",
    "    #test_loader1 = DataLoader(test_dataset,batch_size = 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for data in tqdm(dev_loader):\n",
    "            x,y = data[0].to(device), data[1].to(device) \n",
    "            output   = model(x)\n",
    "            _, predicted  = torch.max(output,1)\n",
    "            #y_pred +=[predicted]\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train 的 cm\n",
    "y_pred, y_true = eval_model(model.to(device), trainloader)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "g = sns.heatmap(cm, annot=True, fmt='d')\n",
    "g.set_xlabel('pred')\n",
    "g.set_ylabel('true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test 的 cm\n",
    "y_pred, y_true = eval_model(model.to(device), testloader)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Confusion matrix : Semi-Supervised')\n",
    "g = sns.heatmap(cm, annot=True, fmt='d')\n",
    "\n",
    "g.set_xlabel('pred')\n",
    "g.set_ylabel('true')\n",
    "#g.set_xticklabels(['A','B','C'])\n",
    "#g.set_yticklabels(['A','B','C'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 : airplain (飛機)\n",
    "1 : automobile (汽車)\n",
    "2 : bird (鳥)\n",
    "3 : cat (貓)\n",
    "4 : deer (鹿)\n",
    "5 : dog (狗)\n",
    "6 : frog (青蛙)\n",
    "7 : horse (馬)\n",
    "8 : ship (船)\n",
    "9 : truck (卡車)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
